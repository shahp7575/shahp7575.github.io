<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Parth's Blog</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2018-09-19T00:00:00-04:00</updated><entry><title>Anscombe's Quartet</title><link href="/anscombes-quartet.html" rel="alternate"></link><published>2018-09-19T00:00:00-04:00</published><updated>2018-09-19T00:00:00-04:00</updated><author><name>Parth Shah</name></author><id>tag:None,2018-09-19:/anscombes-quartet.html</id><summary type="html">&lt;p&gt;I always believed in “numerical calculations are exact, but graphs are rough”. Coming from a person who has just started learning Data Analytics, it was hard for me to understand the importance of Data Visualization along with summary statistics. But it all changed after attending this Data Visualization Meetup, which …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I always believed in “numerical calculations are exact, but graphs are rough”. Coming from a person who has just started learning Data Analytics, it was hard for me to understand the importance of Data Visualization along with summary statistics. But it all changed after attending this Data Visualization Meetup, which is when I was introduced to the &lt;em&gt;Anscombe’s Quartet&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Anscombe’s Quartet was developed by statistician Francis Anscombe. It comprises four datasets, each containing eleven (x,y) pairs. The essential thing to note about these datasets is that they share the same descriptive statistics. But things change completely, and I must emphasize &lt;strong&gt;COMPLETELY&lt;/strong&gt;, when they are graphed. Each graph tells a different story irrespective of their similar summary statistics.&lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="images/summ-stats.png"&gt;&lt;/p&gt;
&lt;p&gt;The summary statistics show that the means and the variances were identical for x and y across the groups :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mean of x is 9 and mean of y is 7.50 for each dataset.&lt;/li&gt;
&lt;li&gt;Similarly, the variance of x is 11 and variance of y is 4.13 for each dataset&lt;/li&gt;
&lt;li&gt;The correlation coefficient (how strong a relationship is between two variables) between x and y is 0.816 for each dataset&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When we plot these four datasets on an x/y coordinate plane, we can observe that they show the same regression lines as well but each dataset is telling a different story :&lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="images/ancombe-plot.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dataset I appears to have clean and well-fitting linear models.&lt;/li&gt;
&lt;li&gt;Dataset II is not distributed normally.&lt;/li&gt;
&lt;li&gt;In Dataset III the distribution is linear, but the calculated regression is thrown off by an outlier.&lt;/li&gt;
&lt;li&gt;Dataset IV shows that one outlier is enough to produce a high correlation coefficient.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This quartet emphasizes the importance of visualization in Data Analysis. Looking at the data reveals a lot of the structure and a clear picture of the dataset.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A computer should make both calculations and graph. Both sorts of output should be studied; each will contribute to understanding.&lt;/p&gt;
&lt;/blockquote&gt;</content></entry><entry><title>Data Preprocessing</title><link href="/data-preprocessing.html" rel="alternate"></link><published>2018-09-14T00:00:00-04:00</published><updated>2018-09-14T00:00:00-04:00</updated><author><name>Parth Shah</name></author><id>tag:None,2018-09-14:/data-preprocessing.html</id><summary type="html">&lt;p&gt;&lt;img alt="image" src="images/ml.png"&gt;&lt;/p&gt;
&lt;p&gt;It was 6 pm on October 17th, 2018, when the wind came crashing at my window. I woke up from my little nap and as usual — I picked up my phone and went to YouTube. Well thanks to YouTube for recommending me this Siraj Raval’s video, where he simply …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="image" src="images/ml.png"&gt;&lt;/p&gt;
&lt;p&gt;It was 6 pm on October 17th, 2018, when the wind came crashing at my window. I woke up from my little nap and as usual — I picked up my phone and went to YouTube. Well thanks to YouTube for recommending me this Siraj Raval’s video, where he simply asks each and every Machine Learning enthusiasts to just dedicate at least one hour of everyday towards studying or coding Machine Learning. I found that motivating as I would have a goal to accomplish every day, rather than having one life-long goal that will take all of my time and effort to reach.&lt;/p&gt;
&lt;p&gt;From the very next moment, I was absolutely devoted to embarking in this one-hour-a-day-ship of Machine Learning. So far my journey has been smooth and I can say with certainty that anyone boarding this ship will definitely reach their destination.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;  If you are just like me, starting with machine learning, then I highly recommend you to take  &lt;a href="https://www.udemy.com/machinelearning/"&gt;this&lt;/a&gt;  course by two professionals of Machine Learning-Kirill Eremenko and Hadelin de Ponteves.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data pre-processing&lt;/strong&gt;  is an important step in the data mining process. Real world data can be incomplete, inconsistent, redundant, noisy or clean (if you’re lucky). Bad data makes the knowledge discovery during the training process very difficult. Hence, we perform data pre-processing so that our machine learning algorithm has a clean training set to train on.&lt;/p&gt;
&lt;p&gt;n this tutorial I will go through a simple example of data pre-processing where we handle missing data, categorical variables and feature scaling using Python.&lt;/p&gt;
&lt;h1&gt;&lt;strong&gt;Step I: Importing libraries&lt;/strong&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;A library is tool that you can use to make a specific job. The best library to import and manage the datasets in Python is Pandas.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;&lt;strong&gt;Step II: Importing the dataset&lt;/strong&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Next, we will read our  &lt;em&gt;custdata.csv&lt;/em&gt; file.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;custdata.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Let’s take a look at our dataset :
&lt;img alt="image" src="images/preprocess-dataset.png"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Looking at the dataset, we can say that we need to handle three things:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Missing Data (NaN)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Categorical Variables (Country, Purchased)&lt;/li&gt;
&lt;li&gt;Feature Scaling&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Step III: Features &amp;amp; Labels&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;We will be using the features(Country, Age &amp;amp; Salary) to predict the labels(Purchased).&lt;/li&gt;
&lt;li&gt;So let’s separate our features and labels as sometimes we don’t want to apply the same transformations to both of them.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dataset_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Purchased&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;dataset_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Step IV: Missing data&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;We can see that our dataset has two missing data. Now, there are three ways in which we can handle this problem:&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;1) Remove the rows that have the missing data&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="sb"&gt;`dataset_features = dataset_features.dropna(subset = [&amp;#39;Age&amp;#39;, &amp;#39;Salary&amp;#39;])`&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="image" src="images/preprocess-features.png"&gt;&lt;/p&gt;
&lt;h2&gt;2) Get rid of the entire column&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dataset_features&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Age&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Salary&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inplace&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="image" src="images/preprocess-nanremoved.png"&gt;&lt;/p&gt;
&lt;h2&gt;3) Replace the missing values with some value(mean, median, zero etc.)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We will use scikit-learn’s  &lt;em&gt;preprocessing&lt;/em&gt; library for this. From this library we will import the  &lt;em&gt;Imputer&lt;/em&gt; class.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Imputer&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Next, we will create an object  &lt;em&gt;imputer&lt;/em&gt; for the class.  &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html"&gt;Click here&lt;/a&gt;  for the parameters of  &lt;em&gt;Imputer&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;imputer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Imputer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;strategy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;As the mean can only be computed on numerical attributes, we will create a copy of our dataset without the text attributes.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dataset_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataset_features&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Age&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Salary&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Now it’s time to  &lt;a href="https://kite.com/python/docs/sklearn.preprocessing.Imputer.fit_transform"&gt;fit and transform&lt;/a&gt;  our  &lt;em&gt;imputer&lt;/em&gt; object to  &lt;em&gt;dataset_num&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dataset_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;imputer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset_num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Let’s take a look at  &lt;em&gt;dataset_num:&lt;/em&gt;
&lt;img alt="image" src="images/preprocess-datanum.png"&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Great! The result  &lt;em&gt;dataset_num&lt;/em&gt; is a numpy array (we can convert into a dataframe) and we can see that the missing values have been replaced by the mean.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;imputer&lt;/em&gt; actually  stores the result of the mean in it  _statistics__ instance variable:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;imputer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;statistics_&lt;/span&gt;  
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;3.36e+01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.16e+04&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="n"&gt;dataset_features&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt; 

&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;3.36e+01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.16e+04&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Step V: Categorical Data&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Our dataset has two categorical variables: Country &amp;amp; Purchased.&lt;/li&gt;
&lt;li&gt;The  &lt;em&gt;Country&lt;/em&gt; contains three categories:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dataset_features&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Country&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="image" src="images/preprocess-unique.png"&gt;&lt;/p&gt;
&lt;p&gt;First, let’s fetch the categorical attributes from our dataset.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dataset_cat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataset_features&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Country&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Now the question is why categorical data needs to be handled?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;It’s because the machine learning models are based on mathematical equations, so if we keep the text or categorical variables in the equation then it will cause some problem. Hence, we encode the categorical variables i.e. encode the text into numbers.(For example: USA = 2, India = 1, Canada = 0)&lt;/li&gt;
&lt;li&gt;However, there’s one problem with this encoding — Since 2&amp;gt;1&amp;gt;0, the equation in the model would think that USA&amp;gt;India&amp;gt;Canada, which we know is not the case at all.&lt;/li&gt;
&lt;li&gt;These are actually three categories and there is no relational order between them. Therefore, in order to prevent the machine learning model from this problem we use dummy variables.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dummy variables are used to sort data into mutually exclusive categories. So instead of one column we will have three separate columns:
&lt;img alt="image" src="images/preprocess-categorical.png"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To create dummy variables we use another class called the  &lt;em&gt;OneHotEncoder&lt;/em&gt; from the scikit-learn  &lt;em&gt;preprocessing&lt;/em&gt; library.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;OneHotEncoder&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Next, we create an object  &lt;em&gt;onehotencoder&lt;/em&gt; for the class OneHotEncoder. Then we fit and transform to our categorical attribute dataset_cat.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;onehotencoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;OneHotEncoder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="n"&gt;dataset_cat_encoded&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;onehotencoder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset_cat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toarray&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The toarray() method is applied just to get a dense NumPy array, else the output would be a sparse matrix.
&lt;img alt="image" src="images/preprocess-catencoded.png"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We do the same for our  &lt;em&gt;Purchased&lt;/em&gt; column. But since it is the dependent variable, the machine learning model will know that it is a category. Hence, there’s no need for one-hot encoding. We can simply encode them into integers using the  &lt;em&gt;OrdinalEncoder&lt;/em&gt; class.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;OrdinalEncoder&lt;/span&gt;

&lt;span class="n"&gt;ordencoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;OrdinalEncoder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;span class="n"&gt;dataset_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ordencoder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset_labels&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="image" src="preprocess/ordencoded.png"&gt;
Awesome!&lt;/p&gt;
&lt;h1&gt;Step VI: Feature Scaling&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;If we look at the age and the salary columns, we can see that they are not on the same scale. The age is ranging from 25 to 56, while the salary is in a range from 28k to 75k.&lt;/li&gt;
&lt;li&gt;Many machine algorithms don’t perform well when the numerical attributes of the independent variables have very different scales. The two common ways of scaling the data are: min-max scaling(normalization) and standardization.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Min-Max Scaling&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Here all the values are subtracted by the minimum value and then divided by maximum minus the minimum. In this approach the data is usually scaled to a fixed range of 0 to 1.&lt;/li&gt;
&lt;li&gt;To implement min-max scaling, we use the  &lt;em&gt;MinMaxScaler&lt;/em&gt; by scikit-learn. It also has a  &lt;em&gt;feature_range&lt;/em&gt; hyperparameter which can be used if we don’t want our scaling to be from 0 to 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;MinMaxScaler&lt;/span&gt;

&lt;span class="n"&gt;scaler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MinMaxScaler&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;span class="n"&gt;dataset_num_scaled&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scaler&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset_num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;dataset_num_scaled&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="image" src="images/preprocess-minmax.png"&gt;&lt;/p&gt;
&lt;h2&gt;Standardization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In standardization, all the values are subtracted by the mean and then it is divided by the variance. Hence, to have features with zero mean and a unit variance.&lt;/li&gt;
&lt;li&gt;To implement standardization, we use the  &lt;em&gt;StandardScaler&lt;/em&gt; transformer from Scikit-learn.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;StandardScaler&lt;/span&gt;

&lt;span class="n"&gt;std_scaler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StandardScaler&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;span class="n"&gt;dataset_num_scaled&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std_scaler&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset_num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;dataset_num_scaled&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="image" src="images/preprocess-standardization.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Note that unlike min-max scaling, standardization cannot guarantee balanced feature scales in the presence of outliers.&lt;/li&gt;
&lt;li&gt;However, in the presence of an outlier the min-max scaling can compress all the inliers in a narrow range. Whereas standardization isn’t much affected by outliers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As we can see that many data transformation steps are required and they all must be executed in the right order. We can actually create a Pipeline to have a single interface for all the transformations.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.pipeline&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;

&lt;span class="n"&gt;num_attribs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Age&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Salary&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  
&lt;span class="n"&gt;cat_attribs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Country&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;num_pipeline&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;  
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;imputer&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Imputer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;strategy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;  
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;std_scaler&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;StandardScaler&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;  
&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;cat_pipeline&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;  
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;onehotencoder&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;OneHotEncoder&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;  
&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.compose&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ColumnTransformer&lt;/span&gt;

&lt;span class="n"&gt;full_pipeline&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ColumnTransformer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;transformers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;  
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;numeric&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_pipeline&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_attribs&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;  
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cat_pipeline&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cat_attribs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;And then we can simply run the whole pipeline and get the results:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dataset_prepared&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;full_pipeline&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset_features&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="image" src="images/preprocess-pipeline.png"&gt;&lt;/p&gt;
&lt;h1&gt;Step VII: Training Set &amp;amp; Test Set&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;We build our machine learning model on a dataset i.e. the  &lt;em&gt;training set&lt;/em&gt; and then we test the performance of our model on the  &lt;em&gt;test set&lt;/em&gt;. For this, we will use the scikit-learn’s  &lt;em&gt;train_test_split()&lt;/em&gt;  function.&lt;/li&gt;
&lt;li&gt;It has few parameters, but the one we will be using is the  &lt;em&gt;random_state_to set the random generator seed. Another parameter is the  _test_size&lt;/em&gt;. Here will choose it as 20%(0.2).&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset_prepared&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Now we can say that our data is ready for training our model on! In real life however, datasets are HUGE but I believe this can be a good starting point to practice and learn and implement the same strategy on large datasets.&lt;/p&gt;</content></entry></feed>